# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

''' Cluster Overview

This view lists the namespaces and the nodes that are available on the current cluster.

'''
import px


# Flag to filter out health checks from the data.
filter_health_checks = True

# Whether or not to include traffic from IPs that don't resolve to a known pod/service.
include_ips = True


# Hack to get the time window for the script.
# TODO(philkuz): Replace this with a built-in.
def get_time_window(start_time: int):
    ''' Converts the start_time string into a table with a single column and single row.
    The approach is hacky, and will round to roughly 1 second.
    '''
    df = px.DataFrame('process_stats', start_time=start_time)

    df = df.agg(
        time_min=('time_', px.min),
        time_max=('time_', px.max),
    )

    df.window = px.DurationNanos(df.time_max - df.time_min)
    df = df[['window']]

    return df


def add_time_window_column(df, start_time):
    tw = get_time_window(start_time)
    df = df.merge(tw, how='inner', left_on=[], right_on=[])
    return df


def process_stats_by_entity(start_time: int, entity: str):
    ''' Gets the windowed process stats (CPU, memory, etc) per node or pod.
    Args:
    @start_time Starting time of the data to examine.
    @entity: Either pod or node_name.
    '''
    # Window size to use on time_ column for bucketing.
    ns_per_s = 1000 * 1000 * 1000
    window_ns = px.DurationNanos(10 * ns_per_s)

    df = px.DataFrame(table='process_stats', start_time=start_time)
    df[entity] = df.ctx[entity]
    df.timestamp = px.bin(df.time_, window_ns)
    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby([entity, 'upid', 'timestamp']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
    )
    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.read_bytes = df.read_bytes_max - df.read_bytes_min
    df.write_bytes = df.write_bytes_max - df.write_bytes_min
    df.rchar_bytes = df.rchar_bytes_max - df.rchar_bytes_min
    df.wchar_bytes = df.wchar_bytes_max - df.wchar_bytes_min
    # Sum by UPID.
    df = df.groupby([entity, 'timestamp']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        read_bytes=('read_bytes', px.sum),
        write_bytes=('write_bytes', px.sum),
        rchar_bytes=('rchar_bytes', px.sum),
        wchar_bytes=('wchar_bytes', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
    )
    df.actual_disk_read_throughput = df.read_bytes / window_ns
    df.actual_disk_write_throughput = df.write_bytes / window_ns
    df.total_disk_read_throughput = df.rchar_bytes / window_ns
    df.total_disk_write_throughput = df.wchar_bytes / window_ns
    # Now take the mean value over the various timestamps.
    df = df.groupby(entity).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.mean),
        cpu_utime_ns=('cpu_utime_ns', px.mean),
        actual_disk_read_throughput=('actual_disk_read_throughput', px.mean),
        actual_disk_write_throughput=('actual_disk_write_throughput', px.mean),
        total_disk_read_throughput=('total_disk_read_throughput', px.mean),
        total_disk_write_throughput=('total_disk_write_throughput', px.mean),
        avg_rss=('rss', px.mean),
        avg_vsize=('vsize', px.mean),
    )
    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / window_ns)
    return df.drop(['cpu_ktime_ns', 'cpu_utime_ns'])


def nodes_for_cluster(start_time: int):
    ''' Gets a list of nodes in the current cluster since `start_time`.
    Args:
    @start_time Start time of the data to examine.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.node = df.ctx['node_name']
    df.pod = df.ctx['pod_name']
    agg = df.groupby(['node', 'pod']).agg()
    nodes = agg.groupby('node').agg(pod_count=('pod', px.count))
    process_stats = process_stats_by_entity(start_time, 'node')
    output = process_stats.merge(nodes, how='inner', left_on='node', right_on='node',
                                 suffixes=['', '_x'])
    return output[['node', 'cpu_usage', 'pod_count']]


def pods_for_cluster(start_time: int):
    ''' A list of pods in `namespace`.
    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The name of the namespace to filter on.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.pod = df.ctx['pod_name']
    df.node = df.ctx['node_name']
    df.container = df.ctx['container_name']
    df = df.groupby(['pod', 'node', 'container']).agg()
    df = df.groupby(['pod', 'node']).agg(container_count=('container', px.count))
    df.start_time = px.pod_name_to_start_time(df.pod)
    df.status = px.pod_name_to_status(df.pod)
    process_stats = process_stats_by_entity(start_time, 'pod')
    output = process_stats.merge(df, how='inner', left_on='pod', right_on='pod',
                                 suffixes=['', '_x'])
    return output[['pod', 'cpu_usage', 'total_disk_read_throughput',
                   'total_disk_write_throughput', 'container_count',
                   'node', 'start_time', 'status']]


def namespaces_for_cluster(start_time: int):
    ''' Gets a overview of namespaces in the current cluster since `start_time`.
    Args:
    @start_time Start time of the data to examine.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.service = df.ctx['service_name']
    df.pod = df.ctx['pod_name']
    df.namespace = df.ctx['namespace']
    agg = df.groupby(['service', 'pod', 'namespace']).agg()
    pod_count = agg.groupby(['namespace', 'pod']).agg()
    pod_count = pod_count.groupby('namespace').agg(pod_count=('pod', px.count))
    svc_count = agg.groupby(['namespace', 'service']).agg()
    svc_count = svc_count.groupby('namespace').agg(service_count=('service', px.count))
    pod_and_svc_count = pod_count.merge(svc_count, how='inner',
                                        left_on='namespace', right_on='namespace',
                                        suffixes=['', '_x'])
    process_stats = process_stats_by_entity(start_time, 'namespace')
    output = process_stats.merge(pod_and_svc_count, how='inner', left_on='namespace',
                                 right_on='namespace', suffixes=['', '_y'])
    return output[['namespace', 'pod_count', 'service_count', 'avg_vsize', 'avg_rss']]


def services_for_cluster(start_time: int):
    ''' Get an overview of the services in the current cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.service = df.ctx['service']
    df = df[df.service != '']
    df.pod = df.ctx['pod']
    df = df.groupby(['service', 'pod']).agg()
    df = df.groupby('service').agg(pod_count=('pod', px.count))
    service_let = service_let_summary(start_time)
    joined = df.merge(service_let, how='left', left_on='service', right_on='service',
                      suffixes=['', '_x'])
    return joined.drop('service_x')


def http_stats(start_time: int):
    ''' Get a dataframe of HTTP events.
    Certain traffic (like health checks) are auto removed, and some standard fields are added.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = px.DataFrame(table='http_events', start_time=start_time)

    # Add K8s metadata.
    df.service = df.ctx['service']
    df.pod = df.ctx['pod']

    # Filter out non-k8s entities.
    df = df[df.pod != '']

    # Additional HTTP fields, pre-computed for convenience.
    df.failure = df.resp_status >= 400

    # Remove health checks, and anything with no remote address.
    health_check_req = ((df.req_path == '/healthz' or df.req_path == '/readyz') or df.req_path == '/livez')
    filter_out_conds = (health_check_req and filter_health_checks) or (df['remote_addr'] == '-')
    df = df[not filter_out_conds]

    return df


def http_stats_by_service(start_time: int):
    ''' Get a data frame of HTTP stats per service. The HTTP stats are for inbound traffic,
    and includes HTTP request count, error count and latency quantiles.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = http_stats(start_time)

    # Filter only to inbound service traffic (server-side).
    # Don't include traffic initiated by this service to an external location.
    df = df[df.trace_role == 2]

    # Compute HTTP metrics.
    df = df.groupby(['service']).agg(
        http_req_count_in=('latency', px.count),
        http_error_count_in=('failure', px.sum),
        http_latency_in=('latency', px.quantiles)
    )

    return df


def conn_stats(start_time: int):
    ''' Get a dataframe of connection stats.
    For each client-server pair, the resulting data frame has the bytes sent and received.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = px.DataFrame(table='conn_stats', start_time=start_time)

    df.pod = df.ctx['pod']
    df.service = df.ctx['service']

    df = df[df.service != '']

    # Find min/max bytes transferred over the selected time window per pod.
    df = df.groupby(['upid', 'remote_addr', 'remote_port', 'pod', 'service', 'trace_role']).agg(
        bytes_recv_min=('bytes_recv', px.min),
        bytes_recv_max=('bytes_recv', px.max),
        bytes_sent_min=('bytes_sent', px.min),
        bytes_sent_max=('bytes_sent', px.max),
    )

    # Calculate bytes transferred over the time window
    df.bytes_sent = df.bytes_sent_max - df.bytes_sent_min
    df.bytes_recv = df.bytes_recv_max - df.bytes_recv_min
    df = df.drop(['bytes_recv_min', 'bytes_recv_max', 'bytes_sent_min', 'bytes_sent_max'])

    return df


def conn_stats_by_service(start_time: int):
    ''' Get a dataframe of connection stats aggregated by service.
    For each service, the resulting data frame contains rx/tx stats for server-side and client-side connections.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = conn_stats(start_time)

    # Group by service and trace role.
    # Do this after computing bytes sent/received by conn_stats key ({upid, remote_addr, remote_port}).
    # Keeping trace_role allows us to see which traffic was part of server duties vs client duties.
    df = df.groupby(['service', 'trace_role']).agg(
        bytes_recv=('bytes_recv', px.sum),
        bytes_sent=('bytes_sent', px.sum),
    )

    # Get RX/TX stats for the server side connections.
    server_df = df[df.trace_role == 2]
    server_df.rx_server = server_df.bytes_recv
    server_df.tx_server = server_df.bytes_sent
    server_df = server_df[['service', 'rx_server', 'tx_server']]

    # Get RX/TX stats for the client side connections.
    client_df = df[df.trace_role == 1]
    client_df.rx_client = client_df.bytes_recv
    client_df.tx_client = client_df.bytes_sent
    client_df = client_df[['service', 'rx_client', 'tx_client']]

    # Create a dataframe that contains both server-side and client-side RX/TX stats.
    df = server_df.merge(client_df,
                         how='left',
                         left_on='service',
                         right_on='service',
                         suffixes=['', '_x'])
    df = df['service', 'rx_server', 'tx_server', 'rx_client', 'tx_client']

    return df


def service_let_summary(start_time: int):
    ''' Compute a summary of traffic by requesting service, for requests
        on services in the current cluster..
    Args:
    @start_time: The timestamp of data to start at.
    '''
    conn_stats_df = conn_stats_by_service(start_time)
    http_stats_df = http_stats_by_service(start_time)

    # Merge conn_stats_df and http_stats_df.
    df = conn_stats_df.merge(http_stats_df,
                             how='left',
                             left_on='service',
                             right_on='service',
                             suffixes=['', '_x'])

    # Compute time window for the query and add it as a column.
    df = add_time_window_column(df, start_time)

    # Compute throughput values.
    df.http_req_throughput_in = df.http_req_count_in / df.window
    df.http_error_rate_in = px.Percent(
        px.select(df.http_req_count_in != 0, df.http_error_count_in / df.http_req_count_in, 0.0))
    df.inbound_conns = (df.rx_server + df.tx_server) / df.window
    df.outbound_conns = (df.tx_client + df.rx_client) / df.window

    return df[['service', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',
               'inbound_conns', 'outbound_conns']]


def service_let_graph(start_time: int):
    ''' Compute a summary of traffic by requesting service, for requests on services
        in the current cluster. Similar to `service_let_summary` but also breaks down
        by pod in addition to service.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = http_stats(start_time)
    df = df.groupby(['service', 'remote_addr', 'pod', 'trace_role']).agg(
        http_req_count_in=('latency', px.count),
        http_error_count_in=('failure', px.sum),
        latency_quantiles=('latency', px.quantiles),
        inbound_bytes_total=('req_body_size', px.sum),
        outbound_bytes_total=('resp_body_size', px.sum)
    )

    # Get the traced and remote pod/service/IP information.
    df.traced_pod = df.pod
    df.traced_service = df.service
    df.traced_ip = px.pod_name_to_pod_ip(df.pod)
    df.remote_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
    df.remote_service = px.service_id_to_service_name(px.ip_to_service_id(df.remote_addr))
    df.remote_ip = df.remote_addr
    # If external IPs are excluded in the service graph, then we also exclude any
    # traffic where we don't know the remote pod or remote service name.
    df = df[include_ips or (df.remote_pod != '' or df.remote_service != '')]

    # Associate it with Client/Server roles, based on the trace role.
    df.is_server_side_tracing = df.trace_role == 2
    df.responder_pod = px.select(df.is_server_side_tracing, df.traced_pod, df.remote_pod)
    df.requestor_pod = px.select(df.is_server_side_tracing, df.remote_pod, df.traced_pod)
    df.responder_service = px.select(df.is_server_side_tracing, df.traced_service, df.remote_service)
    df.requestor_service = px.select(df.is_server_side_tracing, df.remote_service, df.traced_service)
    df.responder_ip = px.select(df.is_server_side_tracing, df.traced_ip, df.remote_ip)
    df.requestor_ip = px.select(df.is_server_side_tracing, df.remote_ip, df.traced_ip)

    # Compute statistics about each edge of the service graph.
    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df = add_time_window_column(df, start_time)
    df.request_throughput = df.http_req_count_in / df.window
    df.inbound_throughput = df.inbound_bytes_total / df.window
    df.outbound_throughput = df.outbound_bytes_total / df.window
    df.error_rate = px.Percent(df.http_error_count_in / df.http_req_count_in)

    df = df.groupby(['responder_pod', 'requestor_pod', 'responder_service',
                     'requestor_service', 'responder_ip', 'requestor_ip']).agg(
        latency_p50=('latency_p50', px.mean),
        latency_p90=('latency_p90', px.mean),
        latency_p99=('latency_p99', px.mean),
        request_throughput=('request_throughput', px.mean),
        error_rate=('error_rate', px.mean),
        inbound_throughput=('inbound_throughput', px.mean),
        outbound_throughput=('outbound_throughput', px.mean),
        throughput_total=('http_req_count_in', px.sum)
    )

    return df

output = pods_for_cluster(__time_from)
output.cpu_usage_percent = output.cpu_usage * 100 
output.total_disk_read_throughput_kb_sec =  output.total_disk_read_throughput * 1024 * 1024
output.total_disk_write_throughput_kb_sec = output.total_disk_write_throughput * 1024 * 1024
phase = px.pluck(output.status, "phase") 
ready = px.pluck(output.status, "ready")  

output.phase = px.pluck(output.status, "phase") 
output.ready = px.pluck(output.status, "ready") 

output = output[['pod', 'phase', 'ready', 'cpu_usage_percent', 'total_disk_read_throughput_kb_sec', 'total_disk_write_throughput_kb_sec', 'container_count', 'node', 'start_time']]
px.display(output)